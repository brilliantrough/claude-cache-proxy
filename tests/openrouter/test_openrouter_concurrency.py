#!/usr/bin/env python3
"""
æµ‹è¯• OpenRouter Proxy æœåŠ¡å™¨çš„å¹¶å‘å¤„ç†èƒ½åŠ›
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import asyncio
import aiohttp
import time
import json
from typing import List, Dict
import statistics
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def get_openrouter_proxy_url():
    """è·å–OpenRouterä»£ç†æœåŠ¡å™¨URL"""
    host = os.getenv('OPENAI_PROXY_HOST', 'localhost')
    port = os.getenv('OPENAI_PROXY_PORT', '9998')
    return f"http://{host}:{port}"

async def single_request(session: aiohttp.ClientSession, request_id: int, url: str) -> Dict:
    """å‘é€å•ä¸ªè¯·æ±‚"""
    start_time = time.time()

    request_data = {
        "model": "anthropic/claude-4.5-sonnet",
        "max_tokens": 20,
        "messages": [
            {"role": "user", "content": f"å¹¶å‘æµ‹è¯•è¯·æ±‚ #{request_id}ï¼Œè¯·ç®€çŸ­å›å¤"}
        ]
    }

    try:
        async with session.post(
            url,
            json=request_data,
            headers={
                "Content-Type": "application/json",
                "Authorization": "Bearer test-key"
            }
        ) as response:
            response_time = time.time() - start_time

            # è¯»å–å“åº”
            response_text = await response.text()

            return {
                "request_id": request_id,
                "status_code": response.status,
                "response_time": response_time,
                "success": response.status == 200,
                "response_text": response_text[:100] + "..." if len(response_text) > 100 else response_text,
                "error": None
            }

    except Exception as e:
        return {
            "request_id": request_id,
            "status_code": None,
            "response_time": time.time() - start_time,
            "success": False,
            "response_text": None,
            "error": str(e)
        }

async def single_stream_request(session: aiohttp.ClientSession, request_id: int, url: str) -> Dict:
    """å‘é€å•ä¸ªæµå¼è¯·æ±‚"""
    start_time = time.time()

    request_data = {
        "model": "anthropic/claude-4.5-sonnet",
        "max_tokens": 50,
        "stream": True,
        "messages": [
            {"role": "user", "content": f"æµå¼å¹¶å‘æµ‹è¯• #{request_id}ï¼Œè¯·ç®€çŸ­å›å¤"}
        ]
    }

    try:
        async with session.post(
            url,
            json=request_data,
            headers={
                "Content-Type": "application/json",
                "Authorization": "Bearer test-key"
            }
        ) as response:
            response_time = time.time() - start_time

            # è¯»å–æµå¼å“åº”
            chunks = []
            async for chunk in response.content:
                if chunk:
                    chunks.append(chunk.decode('utf-8', errors='ignore'))

            full_response = ''.join(chunks)

            return {
                "request_id": request_id,
                "status_code": response.status,
                "response_time": response_time,
                "success": response.status == 200,
                "response_text": full_response[:200] + "..." if len(full_response) > 200 else full_response,
                "error": None
            }

    except Exception as e:
        return {
            "request_id": request_id,
            "status_code": None,
            "response_time": time.time() - start_time,
            "success": False,
            "response_text": None,
            "error": str(e)
        }

async def run_concurrency_test(concurrency: int, total_requests: int, is_stream: bool = False) -> Dict:
    """è¿è¡Œå¹¶å‘æµ‹è¯•"""
    print(f"\nğŸš€ æµ‹è¯•å¹¶å‘èƒ½åŠ›: {concurrency} ä¸ªå¹¶å‘ï¼Œæ€»å…± {total_requests} ä¸ªè¯·æ±‚")
    print(f"ğŸ“ è¯·æ±‚ç±»å‹: {'æµå¼' if is_stream else 'éæµå¼'}")

    url = f"{get_openrouter_proxy_url()}/v1/chat/completions"

    # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(concurrency)

    async def limited_request(session, request_id):
        async with semaphore:
            if is_stream:
                return await single_stream_request(session, request_id, url)
            else:
                return await single_request(session, request_id, url)

    start_time = time.time()

    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [limited_request(session, i) for i in range(1, total_requests + 1)]

        print(f"âœ… å·²åˆ›å»º {len(tasks)} ä¸ª{'æµå¼' if is_stream else ''}ä»»åŠ¡")

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

    total_time = time.time() - start_time

    # ç»Ÿè®¡ç»“æœ
    successful_requests = [r for r in results if isinstance(r, dict) and r.get('success', False)]
    failed_requests = [r for r in results if isinstance(r, dict) and not r.get('success', False)]

    # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
    response_times = [r['response_time'] for r in successful_requests if 'response_time' in r]

    avg_response_time = statistics.mean(response_times) if response_times else 0
    min_response_time = min(response_times) if response_times else 0
    max_response_time = max(response_times) if response_times else 0

    # è®¡ç®—QPS
    requests_per_second = total_requests / total_time if total_time > 0 else 0

    success_rate = len(successful_requests) / total_requests * 100 if total_requests > 0 else 0

    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š {'æµå¼' if is_stream else ''}è¯·æ±‚ç»“æœ:")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({len(successful_requests)}/{total_requests})")
    print(f"   QPS: {requests_per_second:.2f}")
    if response_times:
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
        print(f"   å“åº”æ—¶é—´èŒƒå›´: {min_response_time:.2f}s - {max_response_time:.2f}s")

    if failed_requests:
        print(f"   å¤±è´¥åŸå› : {failed_requests[0].get('error', 'Unknown')[:50]}...")

    return {
        "concurrency": concurrency,
        "total_requests": total_requests,
        "total_time": total_time,
        "successful": len(successful_requests),
        "failed": len(failed_requests),
        "success_rate": success_rate,
        "requests_per_second": requests_per_second,
        "avg_response_time": avg_response_time,
        "min_response_time": min_response_time,
        "max_response_time": max_response_time,
        "results": results
    }

async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ§ª OpenRouter ä»£ç†æœåŠ¡å™¨å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ”— æµ‹è¯•åœ°å€: {get_openrouter_proxy_url()}")
    print("=" * 80)

    # é¦–å…ˆæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{get_openrouter_proxy_url()}/health", timeout=5) as response:
                if response.status != 200:
                    print("âŒ OpenRouterä»£ç†æœåŠ¡å™¨æœªæ­£å¸¸è¿è¡Œ")
                    return
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°OpenRouterä»£ç†æœåŠ¡å™¨: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿OpenRouterä»£ç†æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ: python openrouter_proxy.py")
        return

    print("âœ… OpenRouterä»£ç†æœåŠ¡å™¨è¿è¡Œæ­£å¸¸\n")

    # æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {"concurrency": 1, "total": 5, "name": "åŸºå‡†æµ‹è¯•"},
        {"concurrency": 5, "total": 20, "name": "è½»åº¦å¹¶å‘"},
        {"concurrency": 10, "total": 50, "name": "ä¸­åº¦å¹¶å‘"},
        {"concurrency": 20, "total": 100, "name": "é«˜åº¦å¹¶å‘"},
    ]

    all_results = []

    for scenario in test_scenarios:
        print(f"\n============================== {scenario['name']} ==============================")

        # æµ‹è¯•æ™®é€šè¯·æ±‚
        normal_result = await run_concurrency_test(
            scenario["concurrency"],
            scenario["total"],
            is_stream=False
        )

        # æµ‹è¯•æµå¼è¯·æ±‚
        stream_result = await run_concurrency_test(
            scenario["concurrency"],
            min(20, scenario["total"]),  # æµå¼è¯·æ±‚å‡å°‘æ•°é‡ä»¥èŠ‚çœæ—¶é—´
            is_stream=True
        )

        all_results.append({
            "name": scenario["name"],
            "normal": normal_result,
            "stream": stream_result
        })

    # æ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“‹ å¹¶å‘å¤„ç†èƒ½åŠ›æ€»ç»“")
    print("=" * 80)

    max_qps = 0
    fastest_response = float('inf')

    for result in all_results:
        normal = result["normal"]
        stream = result["stream"]

        max_qps = max(max_qps, normal["requests_per_second"], stream["requests_per_second"])

        if normal["avg_response_time"] > 0:
            fastest_response = min(fastest_response, normal["avg_response_time"])
        if stream["avg_response_time"] > 0:
            fastest_response = min(fastest_response, stream["avg_response_time"])

    print(f"ğŸš€ æœ€å¤§ QPS: {max_qps:.2f}")
    print(f"âš¡ æœ€å¿«å¹³å‡å“åº”æ—¶é—´: {fastest_response:.2f}s")
    print("ğŸ‘ å¹¶å‘å¤„ç†èƒ½åŠ›æ­£å¸¸")

    print("\nğŸ’¡ OpenRouterå¹¶å‘ä¼˜åŒ–å»ºè®®:")
    print("   1. è€ƒè™‘ä½¿ç”¨ aiohttp æ›¿ä»£ requests ä»¥æé«˜å¼‚æ­¥æ€§èƒ½")
    print("   2. ä¸ºæ¯ä¸ªåç¨‹åˆ›å»ºç‹¬ç«‹çš„ HTTP å®¢æˆ·ç«¯å®ä¾‹")
    print("   3. æ·»åŠ è¿æ¥æ± é…ç½®ä»¥ç®¡ç†å¹¶å‘è¿æ¥")
    print("   4. ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶å¢åŠ è¶…æ—¶é…ç½®")
    print("   5. è€ƒè™‘æ·»åŠ è¯·æ±‚é™æµæœºåˆ¶é˜²æ­¢è¿‡è½½")

if __name__ == "__main__":
    asyncio.run(main())
